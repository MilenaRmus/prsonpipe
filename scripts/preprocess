#!/bin/bash
#
# analyze.sh runs the analysis of a subject
# bash analyze "${var[@]}"
# bash analyze 1 2
# bash analyze {1..3}
# bash analyze s001 s005
#
# original author: mason simon (mgsimon@princeton.edu)
# this script was provided by NeuroPipe. modify it to suit your needs
# edited MEW 8/1/16: added conditional statements based on args and added subject loop
# 8/11/16 MEW: finished adding arrays for all prep steps so that it loops through
# array per subject.  Can work with 'none' entries, now.  
# 8/11/16 MEW: updated to go through $TASKS array
# 8/22/16 MEW: included wait_for_it, passes pfiles as arguments to all sbatch commands, 
# 			wd_dir included, copies files from raw to wd_dir_sub
# ------------------------------------------------------------------------------
################################################################################
# Packages used:
#  none
# Arguments:
#  none 	: will run all tasks with all NEW subjects (on a task by task basis)
#  'TSK'	: will run task TSK, if exists (must be first argument, only 1 task)
#  'subs' 	: subject array -- e.g. (1 {3..5}); "${var[@]}"; s001 02 s005; 'all'
################################################################################
#
# SBATCH -J prep-%j
# SBATCH -o ../output/prep-%j.out
# SBATCH -t 00:01:00						# hh:mm:ss

set -e # stop immediately when an error occurs

# load settings and functions
source globals.par	
source funcs
label='[PREP]'

# Run conversion and QA'ing. These scripts just run on data that has not been done yet, 
# so no need for subids run this script and store the job ID output returned by SLURM. 
convert_log=${PROJECT_DIR}/arch/LOG.txt
echo "${label} $(date) -- Converting all dcm files to nii ---" > "${convert_log}"
convert_jobid=$(sbatch -t 240  convert_data | grep -o '[0-9]*')
# runtime set for 240 minutes (time limit for short jobs)

if [[ $QA == 'BXH' ]]; then 
	echo "${label} $(date) -- Running BXH QA ---" >> "${PROJECT_DIR}/qa/LOG.txt"
	sbatch -t 2880 --dependency=afterok:${convert_jobid} bxh_qa
	#runtime set to 48 hours (2880 minutes; max for long job) change this number if necessary
fi

# if only one input, check if task name or subject
if [[ $# -ge 1 ]] && [[ -d ${RAW_DIR}/${1} ]]; then TASKS=(${1}); shift; echo "running task $1 ${TASKS[@]}"; fi

wait_for_it "CONVERT" "${convert_log}"

for t in "${TASKS[@]}"; do 
	echo "${label} >>> Starting task $t <<<"

	# source task's pars file
	if [[ $PREP_SEP -eq 0 ]]; then source ${PROJECT_DIR}/notes/pars.par; else source ${PROJECT_DIR}/notes/pars_${t}.par; fi

	wd_dir_full=${PREP_DIR}/${t}/${wd_dir}
	if [[ ! -d ${wd_dir_full} ]]; then mkdir ${wd_dir_full}; fi

	# call get_subs function
	if [[ $# -eq 0 ]]; then get_subs $t; elif [[ "$1" == 'all' ]]; then get_subs $t 'all'; 
	else declare -a subs_array=( "$@" ); get_subs $t subs_array[@]; fi

	# if no subjects in raw/tsk, then move to next task
	if [[ -z $SUBS ]]; then echo "${label} No subjects found for task $t."; continue; fi

	# Create an array (par_names) of all the preprocessing step names (in order of execution)
	declare -a par_names=("QA" "SLICE_TIME" "REALIGN" "UNWARP" "NORM" "SMOOTH_SOFT")
	# Create an array (pars_array) of all the preprocessing step's software (SPM12w, FSL, DARTEL) (same order as par_names)
	declare -a pars_array=("$QA" "$SLICE_TIME" "$REALIGN" "$UNWARP" "$NORM" "$SMOOTH_SOFT" )
	# Create an array (p_array) of all the preprocessing prefixes (same order as par_names)
	declare -a p_array=('q' 'a' 'r' 'u' 'w' 's')

	# Create a similarity array (sim_array) of all the preprocessing parameters, such that:
	# 0 means the parameter uses a different software than the previous step
	# 1 means the parameter uses the same software as the previous step
	i=0; for p in "${pars_array[@]}"; do
		# check if current step is same as curp (current software parameter)
		if [[ $p != 'none' ]]; then 
			if [[ $p == $curp ]]; then sim_array[$i]=1; else sim_array[$i]=0; fi; curp=$p; 
		else sim_array[$i]=1; fi; i=$(($i + 1))
	done
	i=0; for p in "${pars_array[@]}"; do if [[ $p != 'none' ]]; then sim_array[$i]=1; break; fi; i=$(($i + 1)); done

	echo "${label} ***Running subjects ${SUBS[@]} on the following:"
	echo "${label} QA                    = $QA"
	echo "${label} Slice Time Correction = $SLICE_TIME"
	echo "${label} Motion Correction     = $REALIGN"
	echo "${label} Unwarping             = $UNWARP"
	echo "${label} Normalization         = $NORM"
	echo "${label} Smoothing             = $SMOOTH_SOFT (kernel size: $SMOOTH)"
	
	# Run each subject, one at a time
	n=0; for s in "${SUBS[@]}"; do
		if [[ -z $s ]]; then echo "${label} Can't find subject ${s}. Aborting..."; exit; fi
		first=1;

		wd_dir_sub=${wd_dir_full}/${s}
		logfile=${wd_dir_sub}/LOG.txt

		# copy sub folder from raw/tsk to prep/tsk/wd
		if [[ -d "${wd_dir_sub}" ]]; then echo "${label} ${wd_dir_sub} already exists. Deleting and replacing."; rm -rf ${wd_dir_sub}; fi
		cp -r ${RAW_DIR}/${t}/${s} ${wd_dir_sub}

		echo "${label} $(date) == beginning preprocessing of $s ==" >> "$logfile"
		reset_step_par; unset ext; unset pnames

		i=0; for p in "${pars_array[@]}"; do
			# SET UP STEPS
			p_name=${par_names[$i]}; e=${p_array[$i]}
			# if current step is set to 'none', skip and move on to next step
			if [[ $p != 'none' ]]; then 
				# check if current step is QA(BXH) or NORM(DARTEL)
				if [[ ${par_names[$i]} == 'QA' ]] && [[ $QA == 'BXH' ]]; then continue; fi
				if [[ ${par_names[$i]} == 'NORM' ]] && [[ $NORM == 'DARTEL' ]]; then 
					# if not last subject, reset step par and ext
					if [[ $n -ne ${#SUBS[@]}-1 ]]; then reset_step_par; unset ext; fi; 
					break; 
				fi
				# add step extension to current software's (e.g. FSL) extension list and turn on step in step.par
				ext=${ext}${e}; curp=$p; pnames="${pnames} $p_name"
				echo "${p_name}='$curp'" >> ${PROJECT_DIR}/notes/step.par
			fi
			# check if next step uses same software
			# RUN STEPS
			if [[ ${sim_array[$(($i + 1))]} -eq 0 ]]; then 
				echo "${label} $(date) -- running $pnames in $curp analysis on $s --" >> "$logfile"
				# write out pfile based on step.par
				bash ${PROJECT_DIR}/notes/write_pfile_$curp $t $ext
				script_path="${SCRIPT_DIR_FULL}/${curp}"
				run_script="${script_path}/run_${curp}_prep"
				pfile="${script_path}/p_${curp}_temp_${t}${ext}.m"

				# run steps in current software
				pushd ${script_path} > /dev/null   # move into software's script directory, quietly
				if [[ $first -eq 1 ]]; then 
					prev_jobid=$(sbatch -t 01:00:00 "${run_script}" "$pfile" "$s" | grep -o '[0-9]*')
				else
					prev_jobid=$(sbatch -t 01:00:00 --dependency=afterok:${prev_jobid} "${run_script}" "$pfile" "$s" | grep -o '[0-9]*')
				fi
				popd > /dev/null   # return to the previous directory, quietly

				# reset pars for next step
				reset_step_par; unset ext; first=0; unset pnames;
			fi
			i=$(($i + 1))
		done
		n=$(($n + 1))
	done

	# DARTEL
	if [[ $NORM == 'DARTEL' ]]; then
		for s in "${SUBS[@]}"; do
			logfile=${wd_dir_full}/${s}/LOG.txt
			echo "${label} $(date) == beginning group preprocessing of ${SUBS[@]} ==" >> "$logfile"
		done

		# SET UP FOR DARTEL
		i=`expr ${#pars_array[@]} - 2`  # set i to NORM index (second to last)
		p=${pars_array[i]}; p_name=${par_names[$i]}; e=${p_array[$i]}
		# add step extension to current extension list and turn step on in step.par
		ext=${ext}${e}; curp=$p; pnames="${pnames} $p_name"
		echo "${p_name}='$p'" >> ${PROJECT_DIR}/notes/step.par
		# if smoothing also uses DARTEL, add s to extensions and turn on smooth in step.par
		i=$(($i + 1)); 
		if [[ ${sim_array[i]} -eq 1 ]]; then
			p=${pars_array[i]}; p_name=${par_names[$i]}; e=${p_array[$i]}; 
			ext=${ext}${e}; curp=$p; pnames="${pnames} $p_name"
			echo "${p_name}='$p'" >> ${PROJECT_DIR}/notes/step.par
		fi

		# RUN DARTEL
		for s in "${SUBS[@]}"; do
			logfile=${wd_dir_full}/${s}/LOG.txt
			echo "${label} $(date) -- running $pnames in $curp analysis --" >> "$logfile"
		done
		bash ${PROJECT_DIR}/notes/write_pfile_$curp $t $ext
		script_path="${SCRIPT_DIR_FULL}/$curp"
		run_script="${script_path}/run_${curp}_prep"
		pfile="${script_path}/p_${curp}_temp_${t}${ext}.m"
		pushd ${script_path} > /dev/null   	# move into software's script directory, quietly
		if [[ $first -eq 1 ]]; then 
			prev_jobid=$(sbatch -t 01:00:00 "${run_script}" "$pfile" "${SUBS[@]}" | grep -o '[0-9]*')
		else
			prev_jobid=$(sbatch -t 01:00:00 --dependency=afterok:${prev_jobid} "${run_script}" "$pfile" ${SUBS[@]} | grep -o '[0-9]*')
		fi
		popd > /dev/null   					# return to the previous directory, quietly

		# RUN SMOOTHING (if not in DARTEL)
		if [[ ${sim_array[i]} -eq 0 ]]; then
			for s in "${SUBS[@]}"; do
				reset_step_par; unset ext; unset pnames
				p=${pars_array[i]}; p_name=${par_names[$i]}; e=${p_array[$i]}
				ext=${ext}${e}; curp=$p; pnames="${pnames} $p_name"
				echo "${p_name}='$p'" >> ${PROJECT_DIR}/notes/step.par
				logfile=${wd_dir_full}/${s}/LOG.txt
				echo "${label} $(date) -- running $pnames in $curp analysis --" >> "$logfile"
				bash ${PROJECT_DIR}/notes/write_pfile_$curp $t $ext
				script_path="${SCRIPT_DIR_FULL}/$curp"
				run_script="${script_path}/run_${curp}_prep"
				pfile="${script_path}/p_${curp}_temp_${t}${ext}.m"
				pushd ${script_path} > /dev/null   # move into software's script directory, quietly
				sbatch -t 01:00:00 --dependency=afterok:${prev_jobid} "${run_script}" "$pfile" "${s}"
				popd > /dev/null   # return to the previous directory, quietly
			done
		fi
	fi
	# write '[PREP] Done.' in each subject's LOG.txt file
	for s in "${SUBS[@]}"; do
		logfile=${wd_dir_full}/${s}/LOG.txt
		echo "${label} Done. $(date)" >> "$logfile"
	done
done
echo "${label} Done. $(date)"