#!/bin/bash
#
# analyze.sh runs the analysis of a subject
# bash analyze "${var[@]}"
# bash analyze 1 2
# bash analyze {1..3}
# bash analyze s001 s005
#
# original author: mason simon (mgsimon@princeton.edu)
# this script was provided by NeuroPipe. modify it to suit your needs
# edited MEW 8/1/16: added conditional statements based on args and added subject loop
# 8/11/16 MEW: finished adding arrays for all prep steps so that it loops through
# array per subject.  Can work with 'none' entries, now.  
# 8/11/16 MEW: updated to go through $TASKS array
# 8/22/16 MEW: included wait_for_it, passes pfiles as arguments to all sbatch commands
# ------------------------------------------------------------------------------
################################################################################
# Packages used:
#  none
# Arguments (choose 1):
#  none (will run all new subjects)
#  "${var[@]}" OR s001 2 s005 etc. (subject array -- e.g. (1 {3..5}))
#  'all' (will run all subjects)
################################################################################
#
# SBATCH -J analyze-%j
# SBATCH -o ../output/analyze-%j.out
# SBATCH -t 00:01:00						# hh:mm:ss

set -e # stop immediately when an error occurs

# load settings and functions
source globals.par	
source funcs
label='[PREP]'

# Run conversion and QA'ing. These scripts just run on data that has not been done yet, 
# so no need for subids run this script and store the job ID output returned by SLURM. 
echo "${label} --- Converting all dcm files to nii ---"
convert_jobid=$(sbatch -t 240  convert_data | grep -o '[0-9]*')
# runtime set for 240 minutes (time limit for short jobs)

if [[ $QA == 'BXH' ]]; then 
	echo "${label} --- Running QA ---"
	sbatch -t 2880 --dependency=afterok:${convert_jobid} bxh_qa
	#runtime set to 48 hours (2880 minutes; max for long job) change this number if necessary
fi

#wait_for_it "convert" "$OUT_DIR/logfile"

for t in "${TASKS[@]}"; do 
	echo "${label} >>> Starting task $t <<<"
	if [[ $PREP_SEP -eq 0 ]]; then source ../notes/pars.par; else source ../notes/pars_$t.par; fi
	# call get_subs function based on input argument 
	# to get array SUBS of all subject IDs that will be run
	if [[ $# -eq 0 ]]; then get_subs $t; elif [[ "$1" == 'all' ]]; then get_subs $t 'all'; 
	else declare -a subs_array=( "$@" ); get_subs $t subs_array[@]; fi

	if [[ -z $SUBS ]]; then echo "${label} No subjects found for task $t."; continue; fi

	declare SUBS_$t=${SUBS[@]}

	# Create an array (par_names) of all the preprocessing parameter names in order of execution
	declare -a par_names=("QA" "SLICE_TIME" "REALIGN" "UNWARP" "NORM" "SMOOTH_SOFT")
	# Create an array (pars_array) of all the preprocessing parameters in order of execution
	declare -a pars_array=("$QA" "$SLICE_TIME" "$REALIGN" "$UNWARP" "$NORM" "$SMOOTH_SOFT" )
	# Create an array (p_array) of all the parameter prefixes in same order
	declare -a p_array=('q' 'a' 'r' 'u' 'w' 's')

	# Create a similarity array (sim_array) of all the preprocessing parameters, such that:
	# 0 means the parameter uses a different software than the previous step
	# 1 means the parameter uses the same software as the previous step
	i=0
	for p in "${pars_array[@]}"; do
		if [[ $p != 'none' ]]; then 
			# check if current step is same as curp (current software parameter)
			if [[ $p == $curp ]]; then sim_array[$i]=1; else sim_array[$i]=0; fi
			curp=$p; 
		else sim_array[$i]=1; 
		fi
		i=$i+1
	done
	i=0
	for p in "${pars_array[@]}"; do
		if ! [[ $p == 'none' ]]; then sim_array[$i]=1; break; fi
		i=$i+1
	done

	echo "${label} "
	echo "${label} ***Running subjects ${SUBS[@]} on the following:"
	echo "${label} QA                    = $QA"
	echo "${label} Slice Time Correction = $SLICE_TIME"
	echo "${label} Motion Correction     = $REALIGN"
	echo "${label} Unwarping             = $UNWARP"
	echo "${label} Normalization         = $NORM"
	echo "${label} Smoothing             = $SMOOTH_SOFT (kernel size: $SMOOTH)"
	echo "${label} "

	first=1

	n=0
	# Run each subject, one at a time
	for s in "${SUBS[@]}"; do
		if [[ -z $s ]]; then echo "${label} Can't find subject. Aborting..."; exit; fi

		# save subject ID in form s000
		SUBJ=$s

		echo "${label} == beginning analysis of $SUBJ at $(date) =="
		# reset
		reset_step_par
		if [[ $QA == 'BXH' ]]; then first=0; else first=1; fi

		i=0; for p in "${pars_array[@]}"; do
			# if current step is set to 'none', skip and move on to next step
			if [[ $p == 'none' ]]; then 
				echo "${label} Skipping ${par_names[$i]}"; 
			else
				if [[ ${par_names[$i]} == 'QA' ]] && [[ $QA == 'BXH' ]]; then continue; fi
				# if current step is normalization and it's in DARTEL, break out of loop, go to next sub
				if [[ ${par_names[$i]} == 'NORM' ]] && [[ $NORM == 'DARTEL' ]]; then 
					# if not last subject, reset step par and ext
					if [[ $n -ne ${#SUBS[@]}-1 ]]; then 
						echo "${label} Will run normalization and smoothing later."; reset_step_par; unset ext; 
					fi
					break; 
				fi
				echo "${label} Adding ${par_names[$i]} to $p run list"
				# otherwise, note step
				# add step extension to current extension list of consecutive steps using same software
				ext=${ext}${p_array[$i]}
				# turn step on in step.par
				echo "${par_names[$i]}='$p'" >> ${PROJECT_DIR}/notes/step.par
				if [[ $first -eq 1 ]]; then echo "wd_raw='${raw_DIR}/$t'" >> ${PROJECT_DIR}/notes/step.par; fi
				curp=$p
			fi
			if [[ ${sim_array[$i+1]} -eq 0 ]]; then 
				# next step does not use the same software, so run now
				echo "${label} -- Running $curp analysis --"
				# write out pfile based on step.par
				bash ${PROJECT_DIR}/notes/write_pfile_$curp $t $ext
				script_path="${SCRIPT_DIR_FULL}/${curp}"
				pfile="${script_path}/p_${curp}_temp_${t}${ext}.m"

				# run script
				pushd ${script_path} > /dev/null   # move into this directory, quietly
				if [[ $first -eq 1 ]]; then 
					prev_jobid=$(sbatch -t 01:00:00 -D $curp "${script_path}/run_${curp}_prep" "$pfile" "$SUBJ" | grep -o '[0-9]*')
				else
					prev_jobid=$(sbatch -t 01:00:00 -D $curp --dependency=afterok:${prev_jobid} "${script_path}/run_${curp}_prep" "$pfile" "$SUBJ" | grep -o '[0-9]*')
				fi
				popd > /dev/null   # return to the previous directory, quietly

				# reset pars for next step
				reset_step_par
				unset ext
				first=0
			fi
			i=$i+1
		done
		n=$n+1
	done
	echo "${label} == finished indivdual analyses of ${SUBS[@]} at $(date) =="
	# Normalization and Registration in DARTEL
	if [[ $NORM == 'DARTEL' ]]; then
		# set i to the second to last index
		i=${#pars_array[@]}-2
		# run DARTEL normalization
		echo "${label} Adding ${par_names[$i]} to $p run list"
		# add step extension to current extension list of consecutive steps using same software
		ext=${ext}${p_array[$i]}
		curp=${pars_array[$i]}
		# turn step on in step.par
		echo "${par_names[$i]}='${pars_array[$i]}'" >> ${PROJECT_DIR}/notes/step.par
		if [[ $first -eq 1 ]]; then echo "wd_raw='${RAW_DIR}/$t'" >> ${PROJECT_DIR}/notes/step.par; fi
		if [[ ${sim_array[i+1]} -eq 0 ]]; then 
			# last step (smoothing) does not use DARTEL, so run normalization in DARTEL, then smoothing
			echo "${label} -- Running $curp analysis --"
			# write out pfile based on step.par
			bash ${PROJECT_DIR}/notes/write_pfile_$curp $t $ext
			script_path="${SCRIPT_DIR_FULL}/$curp"
			pfile="${script_path}/p_${curp}_temp_${t}${ext}.m"
			# run script
			pushd ${script_path} > /dev/null   # move into this directory, quietly
			if [[ $first -eq 1 ]]; then 
				prev_jobid=$(sbatch -t 01:00:00 "${script_path}/run_${curp}_prep" "$pfile" "${SUBS[@]}" | grep -o '[0-9]*')
			else
				prev_jobid=$(sbatch -t 01:00:00 --dependency=afterok:${prev_jobid} "${script_path}/run_${curp}_prep" "$pfile" ${SUBS[@]} | grep -o '[0-9]*')
			fi
			popd > /dev/null   # return to the previous directory, quietly

			# reset pars for next step
			reset_step_par
			unset ext
		fi
		# run last time (either norm and smoothing, or just smoothing)
		i=$i+1
		if [[ ${pars_array[$i]} == 'none' ]]; then echo "${label} Skipping ${par_names[$i]}"; 
		else
			curp=${pars_array[$i]}
			echo "${label} Adding ${par_names[$i]} to $curp run list"
			# add step extension to current extension list of consecutive steps using same software
			ext=${ext}${p_array[$i]}
			curp=${pars_array[$i]}
			# turn step on in step.par
			echo "${par_names[$i]}='${pars_array[$i]}'" >> ${PROJECT_DIR}/notes/step.par
		fi
		echo "${label} -- Running $curp analysis --"
		# write out pfile based on step.par
		bash ${PROJECT_DIR}/notes/write_pfile_$curp $t $ext
		script_path="${SCRIPT_DIR_FULL}/${curp}"
		pfile="${script_path}/p_${curp}_temp_${t}${ext}.m"
		# run script
		pushd ${script_path} > /dev/null   # move into this directory, quietly
		if [[ $first -eq 1 ]]; then 
			prev_jobid=$(sbatch -t 01:00:00 "${script_path}/run_${curp}_prep" "$pfile" "${SUBS[@]}" | grep -o '[0-9]*')
		else
			sbatch -t 01:00:00 --dependency=afterok:${prev_jobid} "${script_path}/run_${curp}_prep" "$pfile" "${SUBS[@]}"
		fi
		popd > /dev/null   # return to the previous directory, quietly
	fi
	reset_step_par
	echo "${label} Done with $t subjects: ${SUBS[@]}"
done

echo "${label} >>>>>>> Done with all analyses <<<<<<<"
# for t in "${TASKS[@]}"; do 
# 	echo "$t subjects: ${SUBS_${t}[@]}"
# done
#echo "${label} >>>>>>><<<<<<<"
